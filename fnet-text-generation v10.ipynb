{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Install libraries and import","metadata":{"id":"INcbc_YTM0wu"}},{"cell_type":"code","source":"","metadata":{"id":"tB5Y_I89jMB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\n!pip install torch[transformers]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gaqC--Vin3c","outputId":"e9a8af77-e3ed-479c-9428-23ba9c505748","execution":{"iopub.status.busy":"2023-11-20T15:34:26.531030Z","iopub.execute_input":"2023-11-20T15:34:26.531330Z","iopub.status.idle":"2023-11-20T15:34:51.717679Z","shell.execute_reply.started":"2023-11-20T15:34:26.531303Z","shell.execute_reply":"2023-11-20T15:34:51.716593Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.17.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: torch[transformers] in /opt/conda/lib/python3.10/site-packages (2.0.0)\n\u001b[33mWARNING: torch 2.0.0 does not provide the extra 'transformers'\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch[transformers]) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch[transformers]) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch[transformers]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch[transformers]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch[transformers]) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch[transformers]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch[transformers]) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch","metadata":{"id":"z7BDQtHbgHwW","execution":{"iopub.status.busy":"2023-11-20T15:34:51.719945Z","iopub.execute_input":"2023-11-20T15:34:51.720357Z","iopub.status.idle":"2023-11-20T15:34:55.214264Z","shell.execute_reply.started":"2023-11-20T15:34:51.720318Z","shell.execute_reply":"2023-11-20T15:34:55.213454Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"id":"cpSgOyDnNCx9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"59340787-f565-40f8-f2ef-67580fe205d3","execution":{"iopub.status.busy":"2023-11-20T15:34:55.215490Z","iopub.execute_input":"2023-11-20T15:34:55.216310Z","iopub.status.idle":"2023-11-20T15:34:55.284760Z","shell.execute_reply.started":"2023-11-20T15:34:55.216268Z","shell.execute_reply":"2023-11-20T15:34:55.283515Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Loading Data","metadata":{"id":"TcbdKHGtM_PQ"}},{"cell_type":"code","source":"from datasets import load_dataset\ndatasets = load_dataset('wikitext','wikitext-2-raw-v1')","metadata":{"id":"8LurSU1rgSm0","execution":{"iopub.status.busy":"2023-11-20T15:34:55.287033Z","iopub.execute_input":"2023-11-20T15:34:55.287436Z","iopub.status.idle":"2023-11-20T15:34:59.459308Z","shell.execute_reply.started":"2023-11-20T15:34:55.287406Z","shell.execute_reply":"2023-11-20T15:34:59.458344Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3c9fd347bec4f94bb80ed22d0bdf050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e060784160c413f9213a80d648708c2"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.90 MiB, post-processed: Unknown size, total: 17.40 MiB) to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"091d42ac916c493a8df200f49b957694"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f1460fd7ad412789a4ca3e70eb8097"}},"metadata":{}}]},{"cell_type":"code","source":"datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJtLNGM4imZ_","outputId":"168ba9d9-2aaa-4e97-b840-e062fdd4c123","execution":{"iopub.status.busy":"2023-11-20T15:34:59.460874Z","iopub.execute_input":"2023-11-20T15:34:59.461341Z","iopub.status.idle":"2023-11-20T15:34:59.468000Z","shell.execute_reply.started":"2023-11-20T15:34:59.461315Z","shell.execute_reply":"2023-11-20T15:34:59.466707Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"YYb5t2_jNLwf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. PREPROCESSING DATA","metadata":{"id":"K-tqR-m_NHOs"}},{"cell_type":"code","source":"import re\ndef preprocess_text(sentence):\n  text = sentence['text'].lower() # lowering the sentence and storing in text vaiable\n  text = re.sub('[^a-z?!.,]',' ',text) # removing other than characters and punctuations\n  text = re.sub('\\s\\s+',' ',text) # removing double spaces\n  sentence['text'] = text\n  return sentence","metadata":{"id":"nJQfm0zki0ia","execution":{"iopub.status.busy":"2023-11-20T15:34:59.468927Z","iopub.execute_input":"2023-11-20T15:34:59.469234Z","iopub.status.idle":"2023-11-20T15:34:59.481376Z","shell.execute_reply.started":"2023-11-20T15:34:59.469158Z","shell.execute_reply":"2023-11-20T15:34:59.480615Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"datasets['train'] = datasets['train'].map(preprocess_text)\ndatasets['test'] = datasets['test'].map(preprocess_text)\ndatasets['validation'] = datasets['validation'].map(preprocess_text)","metadata":{"id":"LaLayNePv61c","execution":{"iopub.status.busy":"2023-11-20T15:34:59.482572Z","iopub.execute_input":"2023-11-20T15:34:59.483282Z","iopub.status.idle":"2023-11-20T15:35:04.341027Z","shell.execute_reply.started":"2023-11-20T15:34:59.483248Z","shell.execute_reply":"2023-11-20T15:35:04.340141Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36718 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7943ab4c4fe426d8d7be879002a757b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4358 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36c319d8629848e196b8b5e5efda620f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3760 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40adcd85187a4672bcf8c39eea4410b6"}},"metadata":{}}]},{"cell_type":"code","source":"datasets['train'] = datasets['train'].filter(lambda x : len(x['text']) > 20)\ndatasets['test'] = datasets['test'].filter(lambda x : len(x['text']) > 20)\ndatasets['validation'] = datasets['validation'].filter(lambda x : len(x['text']) > 20)","metadata":{"id":"szzyfA19xspG","execution":{"iopub.status.busy":"2023-11-20T15:35:04.342442Z","iopub.execute_input":"2023-11-20T15:35:04.342823Z","iopub.status.idle":"2023-11-20T15:35:04.629390Z","shell.execute_reply.started":"2023-11-20T15:35:04.342787Z","shell.execute_reply":"2023-11-20T15:35:04.628443Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/37 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"643c23e2e9614590859eac25011df009"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edea53194a0a468f978d663c8e3c9a8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fbc550bc3d54e2db44a1c56726421ef"}},"metadata":{}}]},{"cell_type":"code","source":"datasets","metadata":{"id":"btKuKpLbwJPp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"23dc354c-383b-4dac-d571-d908cf3bd7ef","execution":{"iopub.status.busy":"2023-11-20T15:35:04.630660Z","iopub.execute_input":"2023-11-20T15:35:04.630979Z","iopub.status.idle":"2023-11-20T15:35:04.638088Z","shell.execute_reply.started":"2023-11-20T15:35:04.630952Z","shell.execute_reply":"2023-11-20T15:35:04.636376Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 2312\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 18794\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 1988\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"MLTW7ssTNN0N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. TOKENIZATION","metadata":{"id":"zAKj9639NOYY"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('vocab-transformers/distilbert-word2vec_256k-MLM_best')","metadata":{"id":"eUmPYWBrF90F","execution":{"iopub.status.busy":"2023-11-20T17:51:28.234860Z","iopub.execute_input":"2023-11-20T17:51:28.235309Z","iopub.status.idle":"2023-11-20T17:51:29.153819Z","shell.execute_reply.started":"2023-11-20T17:51:28.235273Z","shell.execute_reply":"2023-11-20T17:51:29.152945Z"},"trusted":true},"execution_count":117,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b37d284b5c14b388e205b82d4bde2e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/6.20M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9e9197c0fa148c69c4ec9831ffac604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2165d23afc034ff481c96b317224f1f6"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.special_tokens_map","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlnV3SRtGvBu","outputId":"cb4c1eac-a764-4f44-e3eb-07964d91b9b2","execution":{"iopub.status.busy":"2023-11-20T15:35:07.399937Z","iopub.execute_input":"2023-11-20T15:35:07.400771Z","iopub.status.idle":"2023-11-20T15:35:07.407183Z","shell.execute_reply.started":"2023-11-20T15:35:07.400738Z","shell.execute_reply":"2023-11-20T15:35:07.406240Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'unk_token': '[UNK]',\n 'sep_token': '[SEP]',\n 'pad_token': '[PAD]',\n 'cls_token': '[CLS]',\n 'mask_token': '[MASK]'}"},"metadata":{}}]},{"cell_type":"code","source":"vocab_size = len(tokenizer.vocab)","metadata":{"id":"kaPsscckHdoV","execution":{"iopub.status.busy":"2023-11-20T15:35:07.408374Z","iopub.execute_input":"2023-11-20T15:35:07.408647Z","iopub.status.idle":"2023-11-20T15:35:07.432344Z","shell.execute_reply.started":"2023-11-20T15:35:07.408623Z","shell.execute_reply":"2023-11-20T15:35:07.431295Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def tokenize(sentence):\n  sentence = tokenizer(sentence['text'],truncation = True)\n\n  return sentence\n\n#tokenized_inputs = datasets['train'].map(tokenize)\ntokenized_inputs = datasets['test'].map(tokenize)","metadata":{"id":"NCGtPcGHNUuo","execution":{"iopub.status.busy":"2023-11-20T15:35:07.433579Z","iopub.execute_input":"2023-11-20T15:35:07.433958Z","iopub.status.idle":"2023-11-20T15:35:08.801016Z","shell.execute_reply.started":"2023-11-20T15:35:07.433920Z","shell.execute_reply":"2023-11-20T15:35:08.800216Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2312 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc3e839df071428984bec788edea62a0"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_inputs = tokenized_inputs.remove_columns(['text','token_type_ids'])","metadata":{"id":"_-wviCing1Ry","execution":{"iopub.status.busy":"2023-11-20T15:35:08.802298Z","iopub.execute_input":"2023-11-20T15:35:08.802945Z","iopub.status.idle":"2023-11-20T15:35:08.810545Z","shell.execute_reply.started":"2023-11-20T15:35:08.802906Z","shell.execute_reply":"2023-11-20T15:35:08.809187Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\n\nbatch = 16\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True,return_tensors =\"pt\")\ndataloader = DataLoader(tokenized_inputs,batch_size=batch,collate_fn=data_collator)","metadata":{"id":"vKU0UAt4IbbK","execution":{"iopub.status.busy":"2023-11-20T15:35:08.811811Z","iopub.execute_input":"2023-11-20T15:35:08.812192Z","iopub.status.idle":"2023-11-20T15:35:19.923735Z","shell.execute_reply.started":"2023-11-20T15:35:08.812139Z","shell.execute_reply":"2023-11-20T15:35:19.922897Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_inputs","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnqZXxeveuUf","outputId":"9ae06e70-334a-4946-87b7-06824582c91d","execution":{"iopub.status.busy":"2023-11-20T15:35:19.924980Z","iopub.execute_input":"2023-11-20T15:35:19.925560Z","iopub.status.idle":"2023-11-20T15:35:19.931674Z","shell.execute_reply.started":"2023-11-20T15:35:19.925532Z","shell.execute_reply":"2023-11-20T15:35:19.930632Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 2312\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## 4. MODEL","metadata":{"id":"3T-vgP77M_Ga"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.fft as fft\nimport numpy as np\nimport pandas as pd","metadata":{"id":"SUgR3zHjPueb","execution":{"iopub.status.busy":"2023-11-20T15:35:19.933129Z","iopub.execute_input":"2023-11-20T15:35:19.933784Z","iopub.status.idle":"2023-11-20T15:35:19.946329Z","shell.execute_reply.started":"2023-11-20T15:35:19.933749Z","shell.execute_reply":"2023-11-20T15:35:19.945452Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(torch.nn.Module):\n    \"\"\"\n    Pytorch module that creates a positional encoding matrix. This matrix will later be added to the\n    transformer's input embeddings to provide a sense of position of the sequence elements.\n    \"\"\"\n\n    def __init__(self, d_model, max_sequence_length):\n        super().__init__()\n        self.d_model = d_model\n        self.max_sequence_length = max_sequence_length\n        self.positional_encoding = self.create_positional_encoding().to(device)\n\n    def create_positional_encoding(self):\n        \"\"\"\n        Creates a positional encoding matrix of size (max_sequence_length, d_model).\n        \"\"\"\n\n        # Initialize positional encoding matrix\n        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n\n        # Calculate positional encoding for each position and each dimension\n        for pos in range(self.max_sequence_length):\n            for i in range(0, self.d_model, 2):\n                # Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n\n                if i + 1 < self.d_model:\n                    # Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n\n        # Convert numpy array to PyTorch tensor and return it\n        return torch.from_numpy(positional_encoding).float()\n\n    def forward(self, x):\n        \"\"\"\n        Adds the positional encoding to the input embeddings at the corresponding positions.\n        \"\"\"\n        # Add positional encodings to input embeddings. The \":\" indexing ensures we only add positional encodings up\n        # to the length of the sequence in the batch. x.size(0) is the batch size, so this is a way to make sure\n        # we're not adding extra positional encodings.\n        expanded_tensor = torch.unsqueeze(self.positional_encoding, 0).expand(x.size(0), -1, -1).to(device)\n\n        return x.to(device) + expanded_tensor[:,:x.size(1), :]\n\n\n\n","metadata":{"id":"a7t8ItlYZLGS","execution":{"iopub.status.busy":"2023-11-20T15:35:19.947701Z","iopub.execute_input":"2023-11-20T15:35:19.947976Z","iopub.status.idle":"2023-11-20T15:35:19.958752Z","shell.execute_reply.started":"2023-11-20T15:35:19.947952Z","shell.execute_reply":"2023-11-20T15:35:19.957818Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n  def __init__(self, sequence_length, vocab_size, embed_dim):\n    super(PositionalEmbedding, self).__init__()\n    self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n    self.position_embeddings = PositionalEncoding(embed_dim,sequence_length)\n\n  def forward(self, inputs):\n    embedded_tokens = self.token_embeddings(inputs).to(device)\n    embedded_positions = self.position_embeddings(embedded_tokens).to(device)\n    return embedded_positions.to(device)","metadata":{"id":"7qmntCODqWU_","execution":{"iopub.status.busy":"2023-11-20T15:35:19.959846Z","iopub.execute_input":"2023-11-20T15:35:19.960101Z","iopub.status.idle":"2023-11-20T15:35:19.971573Z","shell.execute_reply.started":"2023-11-20T15:35:19.960079Z","shell.execute_reply":"2023-11-20T15:35:19.970734Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"YGPZ-svXqWQU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FNetEncoder(nn.Module):\n\n  def __init__(self,embed_dim, dense_dim):\n    super(FNetEncoder,self).__init__()\n    self.embed_dim = embed_dim\n    self.dense_dim = dense_dim\n    self.dense_proj = nn.Sequential(nn.Linear(self.embed_dim,self.dense_dim), nn.ReLU(), nn.Linear(self.dense_dim,self.embed_dim))\n\n    self.layernorm_1 = nn.LayerNorm(self.embed_dim)\n    self.layernorm_2 = nn.LayerNorm(self.embed_dim)\n\n  def forward(self,inputs):\n\n    fft_result = fft.fft2(inputs)\n\n    #taking real part\n    fft_real = fft_result.real.float()\n\n    proj_input = self.layernorm_1 (inputs + fft_real)\n    proj_output = self.dense_proj(proj_input)\n    return self.layernorm_2(proj_input +proj_output)\n\n\n\n","metadata":{"id":"V6j0leewQwUz","execution":{"iopub.status.busy":"2023-11-20T15:35:19.972715Z","iopub.execute_input":"2023-11-20T15:35:19.973924Z","iopub.status.idle":"2023-11-20T15:35:19.982895Z","shell.execute_reply.started":"2023-11-20T15:35:19.973891Z","shell.execute_reply":"2023-11-20T15:35:19.981997Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class FNetDecoder(nn.Module):\n\n  def __init__(self,embed_dim,dense_dim,num_heads):\n    super(FNetDecoder,self).__init__()\n    self.embed_dim = embed_dim\n    self.dense_dim = dense_dim\n    self.num_heads = num_heads\n\n    self.attention_1 = nn.MultiheadAttention(embed_dim,num_heads,batch_first=True)\n    self.attention_2 = nn.MultiheadAttention(embed_dim,num_heads,batch_first=True)\n\n    self.dense_proj = nn.Sequential(nn.Linear(embed_dim, dense_dim),nn.ReLU(),nn.Linear(dense_dim, embed_dim))\n\n    self.layernorm_1 = nn.LayerNorm(embed_dim)\n    self.layernorm_2 = nn.LayerNorm(embed_dim)\n    self.layernorm_3 = nn.LayerNorm(embed_dim)\n\n  def forward(self, inputs, encoder_outputs, mask=None):\n    causal_mask = nn.Transformer.generate_square_subsequent_mask(inputs.size(1)).to(device)\n #   print(causal_mask.size())\n\n    attention_output_1, _ = self.attention_1(inputs, inputs, inputs, attn_mask=causal_mask)\n    out_1 = self.layernorm_1(inputs + attention_output_1)\n#    print (out_1.size(),encoder_outputs.size())\n    if mask != None:\n      attention_output_2, _ = self.attention_2(out_1, encoder_outputs, encoder_outputs, key_padding_mask =torch.transpose(mask, 0, 1).to(device))\n    else:\n      attention_output_2, _ = self.attention_2(out_1, encoder_outputs, encoder_outputs)\n    out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n    proj_output = self.dense_proj(out_2)\n    return self.layernorm_3(out_2 + proj_output)\n\n","metadata":{"id":"TOTs8hpri1MI","execution":{"iopub.status.busy":"2023-11-20T17:05:39.567438Z","iopub.execute_input":"2023-11-20T17:05:39.568337Z","iopub.status.idle":"2023-11-20T17:05:39.578364Z","shell.execute_reply.started":"2023-11-20T17:05:39.568299Z","shell.execute_reply":"2023-11-20T17:05:39.577443Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"class FNetModel(nn.Module):\n    def __init__(self, max_length, vocab_size, embed_dim, latent_dim, num_heads):\n        super(FNetModel, self).__init__()\n\n        self.encoder_inputs = PositionalEmbedding(max_length,vocab_size, embed_dim)\n        self.encoder1 = FNetEncoder(embed_dim, latent_dim)\n        self.encoder2 = FNetEncoder(embed_dim, latent_dim)\n        self.encoder3 = FNetEncoder(embed_dim, latent_dim)\n        self.encoder4 = FNetEncoder(embed_dim, latent_dim)\n\n\n        self.decoder_inputs = PositionalEmbedding(max_length,vocab_size, embed_dim)\n        self.decoder1 = FNetDecoder(embed_dim, latent_dim, num_heads)\n        self.decoder2 = FNetDecoder(embed_dim, latent_dim, num_heads)\n        self.decoder3 = FNetDecoder(embed_dim, latent_dim, num_heads)\n        self.decoder4 = FNetDecoder(embed_dim, latent_dim, num_heads)\n\n\n        self.dropout = nn.Dropout(0.5)\n        self.dense = nn.Linear(embed_dim, vocab_size)\n        \n    def encoder(self,encoder_inputs):\n        x_encoder = self.encoder_inputs(encoder_inputs)\n        x_encoder = self.encoder1(x_encoder)\n        x_encoder = self.encoder2(x_encoder)\n        x_encoder = self.encoder3(x_encoder)\n        x_encoder = self.encoder4(x_encoder)\n        return x_encoder\n    \n    def decoder(self,decoder_inputs,encoder_output,att_mask):\n        x_decoder = self.decoder_inputs(decoder_inputs)\n        x_decoder = self.decoder1(x_decoder, encoder_output,att_mask) ## HERE for inference\n        x_decoder = self.decoder2(x_decoder, encoder_output,att_mask) ## HERE for inference\n        x_decoder = self.decoder3(x_decoder, encoder_output,att_mask) ## HERE for inference\n        x_decoder = self.decoder4(x_decoder, encoder_output,att_mask) ## HERE for inference\n        decoder_outputs = self.dense(x_decoder)   \n    \n        return decoder_outputs\n\n    def forward(self, encoder_inputs, decoder_inputs,att_mask = None):\n        encoder_output = self.encoder(encoder_inputs)\n        decoder_output = self.decoder(decoder_inputs,encoder_output,att_mask=None)\n        return decoder_output\n","metadata":{"id":"g87BEd1QZNzM","execution":{"iopub.status.busy":"2023-11-20T17:05:42.298027Z","iopub.execute_input":"2023-11-20T17:05:42.298415Z","iopub.status.idle":"2023-11-20T17:05:42.309788Z","shell.execute_reply.started":"2023-11-20T17:05:42.298383Z","shell.execute_reply":"2023-11-20T17:05:42.308745Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# Assuming your constants are defined like this:\nMAX_LENGTH = 512\nVOCAB_SIZE = len(tokenizer.vocab)\nEMBED_DIM = 256\nLATENT_DIM = 100\nNUM_HEADS = 4\n\n# Create an instance of the model\nfnet_model = FNetModel(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM, LATENT_DIM, NUM_HEADS).to(device)\n\n","metadata":{"id":"aQ9-rZ4pZNxY","execution":{"iopub.status.busy":"2023-11-20T17:05:45.475418Z","iopub.execute_input":"2023-11-20T17:05:45.475792Z","iopub.status.idle":"2023-11-20T17:05:46.416754Z","shell.execute_reply.started":"2023-11-20T17:05:45.475758Z","shell.execute_reply":"2023-11-20T17:05:46.415889Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# # Define your optimizer and loss function\noptimizer = torch.optim.Adam(fnet_model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\nepochs = 10\nfor epoch in range(epochs):\n    train_loss = 0\n    for batch in dataloader:\n        encoder_inputs_tensor = batch['input_ids'][:,:-1].to(device)\n        decoder_inputs_tensor = batch['input_ids'][:,1:].to(device)\n      #  print(encoder_inputs_tensor)\n       # print(decoder_inputs_tensor)\n        att_mask = batch['attention_mask'][:,:-1].to(device).to(dtype=bool)\n        optimizer.zero_grad()\n        outputs = fnet_model(encoder_inputs_tensor, decoder_inputs_tensor,att_mask)\n#         print(outputs.size())\n#         print(outputs.view(-1, VOCAB_SIZE).size())\n#         print(decoder_inputs_tensor.size())\n#         print(decoder_inputs_tensor.view(-1).size())\n#       #  break\n        decoder_inputs_tensor.masked_fill(batch['attention_mask'][:,1:].ne(1).to(device), -100).to(device)\n\n        loss = criterion(outputs.view(-1, VOCAB_SIZE), decoder_inputs_tensor.view(-1))\n    #    loss = criterion(input = outputs, target= decoder_inputs_tensor)\n        train_loss = train_loss + loss.item()\n        loss.backward()\n        optimizer.step()\n    print (f\"train_loss : {train_loss}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21L5M-3UiILg","outputId":"fedc0e09-e2fd-408a-f051-2cb2d5bafd57","execution":{"iopub.status.busy":"2023-11-20T17:43:59.994804Z","iopub.execute_input":"2023-11-20T17:43:59.995464Z","iopub.status.idle":"2023-11-20T17:46:57.922466Z","shell.execute_reply.started":"2023-11-20T17:43:59.995412Z","shell.execute_reply":"2023-11-20T17:46:57.921498Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"train_loss : 0.6688331579875921\ntrain_loss : 0.05544484220445156\ntrain_loss : 0.01615841467582868\ntrain_loss : 15.153724075993523\ntrain_loss : 18.581757632637164\ntrain_loss : 0.7130553867173148\ntrain_loss : 0.5788254780454736\ntrain_loss : 0.22383555448323023\ntrain_loss : 0.05019339832870173\ntrain_loss : 0.3218885977730679\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"id":"DTCwvvWGZNuc","outputId":"dda25641-eda0-4f05-905d-41d5764fee1b","execution":{"iopub.status.busy":"2023-11-20T17:03:07.776060Z","iopub.status.idle":"2023-11-20T17:03:07.776541Z","shell.execute_reply.started":"2023-11-20T17:03:07.776310Z","shell.execute_reply":"2023-11-20T17:03:07.776334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n#VOCAB = vectorizer.get_vocabulary()\nMAX_LENGTH =100 # your MAX_LENGTH value\n\ndef decode_sentence(input_sentence, fnet_model):\n    fnet_model.eval()\n\n    with torch.no_grad():\n        tokenized_input_sentence = torch.tensor(tokenizer(preprocess_text(input_sentence)['text'])['input_ids']).to(device)# \n        tokenzied_target_sentence = torch.tensor([101]).to(device) # '[CLS]' token  \n        current_text = preprocess_text(input_sentence)['text']\n        for i in range(MAX_LENGTH):\n            predictions = fnet_model(tokenized_input_sentence[:-1].unsqueeze(0),tokenzied_target_sentence.unsqueeze(0))\n            predicted_index = torch.argmax(predictions[0, -1, :]).item()\n            predicted_token = tokenizer.decode(predicted_index)\n            if predicted_token == \"[SEP]\":  # Assuming [end] is the end token\n              break\n            current_text += \" \"+ predicted_token\n            tokenized_target_sentence = torch.cat([tokenzied_target_sentence, torch.tensor([predicted_index]).to(device)], 0).to(device)\n            tokenized_input_sentence = torch.tensor(tokenizer(current_text)['input_ids']).to(device)\n        return current_text\ndecode_sentence({'text': 'How are you ?'}, fnet_model)","metadata":{"id":"FGIcLS9CsMYa","execution":{"iopub.status.busy":"2023-11-20T17:47:17.244394Z","iopub.execute_input":"2023-11-20T17:47:17.244780Z","iopub.status.idle":"2023-11-20T17:47:17.991932Z","shell.execute_reply.started":"2023-11-20T17:47:17.244748Z","shell.execute_reply":"2023-11-20T17:47:17.990972Z"},"trusted":true},"execution_count":116,"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"'how are you ? next ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##wg ##isan ##wg ##isan ##isan ##isan ball ##wg ##wg ##isan ball ##isan ##isan ball ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ball ball ball ball ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan had ##wg ##isan ##isan ##isan ##isan ##wg ##isan ball ball ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ##isan ball ##isan ##isan ##isan ##isan ##isan ##wg ##isan ##isan ##isan ##isan'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer('')","metadata":{"id":"NiTAmDOOYNBb","execution":{"iopub.status.busy":"2023-11-20T14:17:06.429125Z","iopub.execute_input":"2023-11-20T14:17:06.430028Z","iopub.status.idle":"2023-11-20T14:17:06.436058Z","shell.execute_reply.started":"2023-11-20T14:17:06.429994Z","shell.execute_reply":"2023-11-20T14:17:06.435075Z"},"trusted":true},"execution_count":164,"outputs":[{"execution_count":164,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 102], 'token_type_ids': [0, 0], 'attention_mask': [1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"for batchs in dataloader:\n    print(batchs)\n    break","metadata":{"id":"NvImg6m6YM_G","execution":{"iopub.status.busy":"2023-11-20T17:39:01.473044Z","iopub.execute_input":"2023-11-20T17:39:01.473482Z","iopub.status.idle":"2023-11-20T17:39:01.491265Z","shell.execute_reply.started":"2023-11-20T17:39:01.473444Z","shell.execute_reply":"2023-11-20T17:39:01.490310Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[ 101, 2728, 8945,  ...,    0,    0,    0],\n        [ 101, 1999, 1010,  ...,    0,    0,    0],\n        [ 101, 1999, 8945,  ...,    0,    0,    0],\n        ...,\n        [ 101, 2010, 2269,  ...,    0,    0,    0],\n        [ 101, 1999, 1996,  ...,    0,    0,    0],\n        [ 101, 1999, 1010,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.encode('[CLS]')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"0ZXZ8sHKYM8d","outputId":"087eb32e-83d7-4e6c-9b23-12f1a9b800ba","execution":{"iopub.status.busy":"2023-11-20T15:41:23.980932Z","iopub.execute_input":"2023-11-20T15:41:23.981313Z","iopub.status.idle":"2023-11-20T15:41:23.988081Z","shell.execute_reply.started":"2023-11-20T15:41:23.981282Z","shell.execute_reply":"2023-11-20T15:41:23.986861Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"[101, 101, 102]"},"metadata":{}}]},{"cell_type":"code","source":"[-16.4226, -15.4476,  17.9977, -15.4673, -15.5945, -15.4213, -16.0374,\n        -14.9229, -14.6924, -15.9143]","metadata":{"id":"1c7e6zbZYM5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(tokenized_inputs[0]['input_ids'])","metadata":{"id":"D_h5JKVvsMVY","execution":{"iopub.status.busy":"2023-11-20T17:23:43.064054Z","iopub.execute_input":"2023-11-20T17:23:43.064460Z","iopub.status.idle":"2023-11-20T17:23:43.074141Z","shell.execute_reply.started":"2023-11-20T17:23:43.064423Z","shell.execute_reply":"2023-11-20T17:23:43.073198Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"'[CLS] robert boulter is an english film, television and theatre actor. he had a guest starring role on the television series the bill in. this was followed by a starring role in the play herons written by simon stephens, which was performed in at the royal court theatre. he had a guest role in the television series judge john deed in. in boulter landed a role as craig in the episode teddy s story of the television series the long firm he starred alongside actors mark strong and derek jacobi. he was cast in the theatre productions of the philip ridley play mercury fur, which was performed at the drum theatre in plymouth and the menier chocolate factory in london. he was directed by john tiffany and starred alongside ben whishaw, shane zaza, harry kent, fraser ayres, sophie stanton and dominic hall. [SEP]'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer('[hi this]',padding=True,batch=12)","metadata":{"id":"BOIViVxmsMSy","execution":{"iopub.status.busy":"2023-11-20T17:36:30.783354Z","iopub.execute_input":"2023-11-20T17:36:30.783758Z","iopub.status.idle":"2023-11-20T17:36:31.006574Z","shell.execute_reply.started":"2023-11-20T17:36:30.783727Z","shell.execute_reply":"2023-11-20T17:36:31.005285Z"},"trusted":true},"execution_count":109,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[hi this]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2904\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2885\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2886\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2902\u001b[0m     )\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2907\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2977\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2969\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2970\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2975\u001b[0m )\n\u001b[0;32m-> 2977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    575\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 576\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n","\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'batch'"],"ename":"TypeError","evalue":"PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'batch'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"id":"vNOXdPZfZNU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"dbtJcztOZNSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8n4kHiUeZNPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"9pA2PyG1ZNNg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n#VOCAB = vectorizer.get_vocabulary()\nMAX_LENGTH =10 # your MAX_LENGTH value\n\ndef decode_sentence(input_sentence, fnet_model):\n    fnet_model.eval()\n\n    with torch.no_grad():\n        # Mapping the input sentence to tokens and adding start and end tokens\n        tokenized_input_sentence = torch.tensor(tokenizer(preprocess_text(input_sentence)['text'])['input_ids']).to(device)# )\n        inital_state = fent_model.encoder(tokenized_input_sentence)[:,-1,:]\n        \n        inital_target = torch.tensor([101]).to(device) # start token [CLS]\n\n        current_text = '' \n\n        for i in range(MAX_LENGTH):\n          # Get the predictions\n            if i == 0:\n                predictions = fnet_model.decoder(inital_state,inital_target)\n            predicted_index = torch.argmax(predictions[0, -1, :]).item()\n        \n               \n            predicted_token = tokenizer.decode(predicted_index)\n\n            if predicted_token == \"[sep]\":  # [sep] is the end token\n              break\n            current_text += \" \" + predicted_token\n            initial_target = torch.tensor(tokenizer(current_text)['input_ids'][:-1]).to(device)\n\n        return current_text\n\n#         # Calculating the token with maximum probability and getting the corresponding word\n#         sampled_token_index = torch.argmax(predictions[0, 0, :]).item()\n#  #       sampled_token = tokenizer.vocab[sampled_token_index]\n#         # If sampled token is the end token then stop generating and return the sentence\n#       #  if sampled_token == \"[end]\":\n#        #     break\n#         decoded_sentence += str(sampled_token_index) + \" \"\n#         tokenized_target_sentence = torch.cat(\n#             [tokenized_target_sentence, torch.tensor([sampled_token_index])], 0\n#         )\n\n#     return decoded_sentence\n\n# Assuming you have a PyTorch model named fnet_model\n# You need to pass fnet_model as an argument to the function\n#decode fnet_model = YourPyTorchModel()\ndecode_sentence({'text':'Where have  you all been '}, fnet_model)","metadata":{"id":"Pb0LjSGIZNKb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\nclass FNetTextGenerator(nn.Module):\n  def __init__(self, embed_dim, latent_dim, vocab_size, max_seq_len, num_heads):\n    super(FNetTextGenerator, self).__init__()\n    self.positional = PositionalEmbedding(max_seq_len, vocab_size, embed_dim)\n    self.encoder = FNetEncoder(embed_dim,latent_dim)\n    self.decoder = FNetDecoder(embed_dim, latent_dim, num_heads)\n\n  def forward(self, inputs, target=None):\n    positional_inputs = self.positional(inputs)\n    encoder_output = self.encoder(positional_inputs)\n#    print(encoder_output.size())\n    if target is not None:\n      decoder_output = self.decoder(target, encoder_output)\n      return decoder_output\n\n    # If no target is provided, generate autoregressively\n    batch_size, seq_len = inputs.size()\n  #  print(inputs.size())\n    generated_sequence = torch.zeros(batch_size, seq_len, dtype=torch.long, device=inputs.device)\n\n        # Initial input for autoregressive decoding\n    input_token = inputs[:, 0].unsqueeze(1).float()\n #   print(input_token)\n\n# ...\n\n    for t in range(1, seq_len):\n      decoder_output = self.decoder(positional_inputs, encoder_output)\n\n    # Use torch.argmax directly to get the indices of the maximum values\n      predicted_token = torch.argmax(F.softmax(decoder_output, dim=-1), dim=-1)\n\n          # Check the shape of predicted_token before updating the generated sequence\n      print(f\"Shape of predicted_token before update: {predicted_token.shape}\")\n\n\n    # Ensure that predicted_token has the correct shape\n      predicted_token = predicted_token.unsqueeze(1)\n\n    # Update the generated sequence\n      generated_sequence[:, t] = predicted_token.squeeze()\n\n    # Update input_token for the next iteration\n      input_token = predicted_token\n\n\n\n    return generated_sequence\n\n# Example usage\nembed_dim = 256\nlatent_dim = 512\nvocab_size = 10000\nmax_seq_len = 50\nnum_heads = 8\n\n\n\nmodel = FNetTextGenerator(embed_dim, latent_dim, vocab_size, max_seq_len, num_heads).to(device)\n\n# Dummy input\ninputs = torch.randint(0, vocab_size, (16, max_seq_len)).long().to(device)\n\n\n# Forward pass for text generation\ngenerated_sequence = model(inputs)\nprint(\"Generated Sequence:\", generated_sequence)","metadata":{"id":"6CJl9jzMi1HA","outputId":"5a37f27a-dc34-4eb4-f697-3405591ab7b2","colab":{"base_uri":"https://localhost:8080/","height":408}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"Shape of predicted_token before update: torch.Size([16, 50])\n"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-2b3728802bf9>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Forward pass for text generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated Sequence:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-2b3728802bf9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, target)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Update the generated sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Update input_token for the next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expand(torch.LongTensor{[16, 50]}, size=[16]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"]}]},{"cell_type":"code","source":"","metadata":{"id":"YEf_9tgG6UuQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"6L9MfZl3i1ET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = torch.randint(0, vocab_size, (16, max_seq_len)).to(device)","metadata":{"id":"N4cWLfC7i1Bp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.dtype","metadata":{"id":"ESzOG_3Vi0_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"iIPaRAr4i08m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"IHtD0MZvi06F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"H7unSim_i03p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"wMOlDWTNi00o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FNetTextGenerator(nn.Module):\n    def __init__(self, embed_dim, latent_dim, vocab_size, max_seq_len, num_heads, num_layers):\n        super(FNetTextGenerator, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.positional_embedding = nn.Embedding(max_seq_len, embed_dim)\n        self.transformer_decoder_layer = nn.TransformerDecoderLayer(embed_dim, num_heads)\n        self.transformer_decoder = nn.TransformerDecoder(self.transformer_decoder_layer, num_layers=num_layers)\n\n        self.transformer_encoder_layer = FNetEncoder(embed_dim, latent_dim)\n      #  self.transformer_encoder = torch.nn.TransformerEncoder(self.transformer_encoder_layer, num_layers = num_layers)\n\n        self.fc = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, inputs, target=None):\n        seq_length = inputs.size(1)\n\n        embedded_tokens = self.embedding(inputs)\n        positions = torch.arange(0, seq_length, device=inputs.device).unsqueeze(0)\n        embedded_positions = self.positional_embedding(positions)\n        encoder_input = embedded_tokens + embedded_positions\n\n        encoder_output = self.transformer_encoder_layer(encoder_input)\n\n        if target is not None:\n            target = target.permute(1, 0, 2)  # Adjust the shape for transformer decoder\n            decoder_output = self.transformer_decoder(target, encoder_output)\n            return decoder_output.permute(1, 0, 2)  # Adjust the shape back to batch-first\n\n        # If no target is provided, generate autoregressively\n        batch_size, seq_len = inputs.size()\n        generated_sequence = torch.zeros(batch_size, seq_len, dtype=torch.long, device=inputs.device)\n\n        # Initial input for autoregressive decoding\n        input_token = inputs[:, 0].unsqueeze(1)\n\n        for t in range(1, seq_len):\n            embedded_input_token = self.embedding(input_token) + self.positional_embedding(torch.tensor([[t]]).to(inputs.device))\n            decoder_output = self.transformer_decoder(embedded_input_token, encoder_output)\n            logits = self.fc(decoder_output[-1])\n            _, predicted_token = torch.max(F.softmax(logits, dim=-1), dim=-1)\n            generated_sequence[:, t] = predicted_token.squeeze()\n\n            input_token = predicted_token.unsqueeze(1)\n\n        return generated_sequence\n\n","metadata":{"id":"kbyGefdHazVt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\nembed_dim = 256\nlatent_dim = 512\nvocab_size = 10000\nmax_seq_len = 50\nnum_heads = 8\nnum_layers = 6\n\nmodel = FNetTextGenerator(embed_dim, latent_dim, vocab_size, max_seq_len, num_heads, num_layers).to(device)\n\n# Dummy input\ninputs = torch.randint(0, vocab_size, (2, max_seq_len)).to(device)\n\n# Forward pass for text generation\ngenerated_sequence = model(inputs)\nprint(\"Generated Sequence:\", generated_sequence)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":322},"id":"Ym3LMVT1cekA","outputId":"3a1079e2-541b-4427-bc68-1770a57d73fc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-606cc94ec858>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFNetTextGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Dummy input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: FNetTextGenerator.__init__() takes 6 positional arguments but 7 were given"]}]},{"cell_type":"code","source":"","metadata":{"id":"nIEJYOfRcfTE","colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"3fd1f9b8-fe44-48be-cb42-2993839157ea"},"execution_count":23,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-fe974e565db0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not callable"]}]},{"cell_type":"code","source":"type()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DY9x76ind0qD","outputId":"8a90a015-9931-43f3-de9b-fb7ab3d47e20"},"execution_count":126,"outputs":[{"output_type":"execute_result","execution_count":126,"data":{"text/plain":["torch.Tensor"]},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"4M6xFK8Vn2lT"},"execution_count":null,"outputs":[]}]}